---
title: "EinfÃ¼hrung in R & RStudio â€“ Tag 2"
subtitle: "Fehlende Werte, Umkodieren & Workflows"
format:
  html:
    toc: true
    toc-title: "Tag 2 â€“ Inhalte"
    toc-location: left
    toc-depth: 2

    
sidebar: workshop1
---

Willkommen zurÃ¼ck ğŸ‘‹  
Wir starten mit einem kurzen Recap von Tag 1 und klÃ¤ren offene Fragen.

# Recap - Tag 1
___
Wir haben im ersten Teil des Workshops gelernt, wie RStudio aufgebaut ist und funktioniert. Wir kennen nun die wichtigsten Grundlagen und kÃ¶nnen diese heute weiter **vertiefen und festigen**.

::: callout-tip
# **Recap-Fragen**
- Was ist ein Data Frame?
- Was macht `select()` vs. `filter()`?
- WofÃ¼r nutzen wir Pipes (`|>`)?
:::

::: callout-note
# **Merksatz**

R ist eine **Programmiersprache** â€“ und wie jede Sprache lernt man sie durch **regelmÃ¤ÃŸige Nutzung**.

Fehler zu machen ist vÃ¶llig normal (auch wenn es manchmal frustrierend ist).  
FÃ¼r fast jedes Problem gibt es **eine LÃ¶sung, eine ErklÃ¤rung oder ein Paket** ğŸ˜Š
:::


# Umkodieren
___
In der Datenanalyse arbeiten wir hÃ¤ufig mit Variablen, deren Werte **nicht direkt so vorliegen**, wie wir sie fÃ¼r eine Auswertung benÃ¶tigen.

**Umkodieren** bedeutet, Werte oder Kategorien gezielt zu verÃ¤ndern oder neu zusammenzufassen, damit Variablen

- **inhaltlich korrekt** interpretiert werden kÃ¶nnen und
- **fÃ¼r Analysen** (z. B. Gruppenvergleiche, Skalenbildung) besser nutzbar sind.

Beim Umkodieren kÃ¶nnen zum Beispiel:

- Kategorien zusammengefasst werden  
  (z. B. mehrere BildungsabschlÃ¼sse â†’ â€niedrig / mittel / hochâ€œ)
- Werte umgedreht werden  
  (z. B. hohe Werte = geringe Zustimmung)
- neue Variablen aus bestehenden gebildet werden  
  (z. B. Summen- oder Skalenwerte) **â†’** das kennen wir schon!

::: callout-tip
# **Warum ist Umkodieren wichtig?**

Rohdaten sind selten sofort â€analysefertigâ€œ. Durch Umkodieren stellen wir sicher, dass unsere Variablen **inhaltlich korrekt** interpretiert und **vergleichbar** ausgewertet werden kÃ¶nnen. Umkodierung ist daher ein wichtiger **Bestandteil der Datenaufbereitung** und **wichtig** fÃ¼r die spÃ¤teren Analysen!
:::

Im Folgenden sehen wir typische Beispiele fÃ¼r Umkodierungen und setzen sie direkt mit R um.

## Umkodieren invertierter Items

In FragebÃ¶gen kommen hÃ¤ufig invertierte Items vor. Das bedeutet: Hohe Werte drÃ¼cken geringe Zustimmung aus â€“ oder umgekehrt.

Damit alle Items in dieselbe Richtung zeigen, mÃ¼ssen invertierte Items umkodiert werden.

### Beispiel 

Die Items `hs04` bis `hs07 und hs09` Fragen die mentale Gesundheit in den letzten vier Wochen ab:

|**Variable**|**Inhalt**|**Skalierung**|
|----|----|----|
|hs04|"Hetze, unter Zeitdruck"| 1 = "Immer" bis 5 = "Nie"|
|hs05|"Niedergeschlage"| 1 = "Immer" bis 5 = "Nie"|
|hs06|"Ruhig, Ausgeglichen"| 1 = "Immer" bis 5 = "Nie"|
|hs07|"Jede menge Energie"| 1 = "Immer" bis 5 = "Nie"|
|hs09|"Einsam"|1 = "Immer" bis 5 = "Nie"|

Wie wir sehen, sind einige Items invertiert formuliert. WÃ¼rden wir daraus, eine Skala bilden bspw. **Disstress**, kÃ¶nnten wir mit dem `mutate()`Befehl, keine adÃ¤quate Skala bilden.

Wir mÃ¼ssen also entscheiden, was nun **hohe Werte und was niedrige Werte** darstellen sollen. Damit es intuitiver ist, wollen wir nun die **Variablen `hs04`, `hs05` und `hs09`umkodieren**, damit spÃ¤ter ein hoher Wert auf der Skala **"viel Disstress"** darstellen kann.

Doch bevor wir Anfangen, laden wir zunÃ¤chst unseren Datensatz und die Pakete wieder ein, bzw. haben diese schon im Environment!

```{r}
#| echo: true
#| warning: false

#install.packages("labelled", repos = "https://cloud.r-project.org")

library(tidyverse)
library(haven)
library(labelled)

df_csv <- read_csv("assets/data/datensatz_workshop.csv")

df_spss <- read_sav("assets/data/spss_datensatz_workshop.sav")

head(df_csv, 10)
```

Zum besseren VerstÃ¤ndnis verkleinern wir erstmal unseren Datensatz: Erstelle den Datensatz `df_diss` mit nur den Items die wir benÃ¶tigen.

:::{.callout-tip collapse="true"}
# **LÃ¶sung**

```{r}

df_diss <- df_csv |>
select(hs04,hs05,hs06,hs07,hs09)

head(df_diss, 10)

```

:::

Nun kÃ¶nnen wir in diesem neuen Datensatz, getrennt von den Rohdaten, Ã¼ben!

:::{.panel-tabset}

# Alternative 1

```{r}
#| echo: true

# jede Variable einzeln und manuell

df_diss$hs04_inv <- 6 - df_diss$hs04

head(df_diss$hs04,10)

head(df_diss$hs04_inv,10)
```
:::{.callout-note collapse="true"}
# **Hinweis**
Bei der Intertierung, wird...
- der Wert`1` zu  `5`
- der Wert `2` zu `4`
- der Wert `3`bleibt `3`
- der Wert `4` zu `2`
- der Wert `5` zu `1`

Die oben vorgstellte **Alternative 1** basiert auf der Annahme **"maximaler Skalenwert" + 1 - "Skalenwert der Person"**.
:::

# Alternative 2

```{r}
#| echo: true

 df_diss<-df_diss|>
    mutate(hs04_inv = recode(hs04,
                             "1"=5,
                             "2"=4,
                             "3"=3,
                             "4"=2,
                             "5"=1),
           hs05_inv = recode(hs05,
                             "1"=5,
                             "2"=4,
                             "3"=3,
                             "4"=2,
                             "5"=1),
           hs09_inv = recode(hs09,
                            "1"=5,
                            "2"=4,
                            "3"=3,
                            "4"=2,
                            "5"=1),
           )

head(df_diss, 10)           
```

# Alternative 3

```{r}
#| echo: true
#| warning: false
#| eval: false

install.packages("sjmisc")

```
```{r}
#| echo: true
#| warning: false

library(sjmisc)

df_diss <- df_diss|>
            mutate(hs04_inv = rec(hs04, rec = "rev"),
                  hs05_inv = rec(hs05, rec = "rev"),
                  hs09_inv = rec(hs09, rec = "rev")
                  )

head(df_diss, 10)

```
:::{.callout-note collapse="true"}
# **Hinweis**
Wir haben hier einen Befehl aus einem anderen Paket genutzt. Wie du sehen kannst, sind Pakete auch miteinander kombinierbar!

Hier handelt es sich um den `rec()` Befehl aus dem Paket **sjmisc**.
:::
:::

## Umkodieren von Gruppen und Faktoren

Bisher haben wir Variablen umkodiert, die **numerisch skaliert** sind (z. B. Zustimmungsskalen von 1 bis 5).
In der Praxis arbeiten wir jedoch sehr hÃ¤ufig auch mit **kategorialen Variablen**, zum Beispiel:

- Geschlecht  
- Bildungsabschluss  
- Wohnort  
- Erwerbsstatus  

Diese Variablen sind meist als **Faktoren** kodiert oder sollen als solche behandelt werden.
Beim Umkodieren von Faktorvariablen geht es weniger um Rechnen, sondern darum, **Kategorien sinnvoll zusammenzufassen oder neu zu benennen**.

In diesem Abschnitt schauen wir uns daher an, wie wir **Faktorvariablen gezielt umkodieren**, zum Beispiel indem wir:

- mehrere Kategorien zu einer Gruppe zusammenfassen oder  
- neue, besser interpretierbare Gruppen bilden.
___
Wir erstellen nun erneut einen Datensatz und nennen es `df_rec`. Diesmal fÃ¼gen wir zu unseren Disstress Variablen weitere hinzu. Folgende Variablen sollen hinzugenommen werden:
- `gs01` - Selbsbeschreibung des Wohnorts
- `educ` - Allgemeiner Schulabschluss
- `mstat` - Familienstand
- `im17` bis `im21` - Einstellung zur sozialen Ungleichheit

:::{.callout-important collapse="true"}
# **Beschreibungen der Variablen**

:::{.panel-tabset}
# Wohnort
|**Variable**|**Inhalt**|**Skalierung**|
|----|----|----|
|gs01|"Selbsbeschreibung des Wohnorts"| 1 = GroÃŸstadt<br> 2 = Vorort GroÃŸstadt<br> 3 = Mittel-/Kleinstadt<br> 4 = lÃ¤ndl. Dorf<br> 5 = Allein stehendes Haus auf dem Land |

# Schulabschluss
|**Variable**|**Inhalt**|**Skalierung**|
|----|----|----|
|educ|"Allgemeiner Schulabschluss"| 1 = Ohne Abschluss<br> 2 = Volks-/Hauptschule<br> 3 = Mittelere Reife<br> 4 = Fachhochschulreife<br> 5 = Hochschulreife<br> 6 = Anderer Abschluss<br> 7 = Noch SchÃ¼ler*in |

# Familienstand
|**Variable**|**Inhalt**|**Skalierung**|
|----|----|----|
|mstat|"Familienstand"| 1 = Verheiratet (zsm.lebend)<br> 2 = Verheiratet (getrennt)<br> 3 = Verwitwet<br> 4 = Geschieden<br> 5 = Ledig<br> 6 = Lebensp. (zsm.lebend)<br> 7 = Lebensp. (getrennt)<br> 8 = Lebensp. verstorben<br> Lebensp. aufgehoben |

# Soziale Ungleichheit
|**Variable**|**Inhalt**|**Skalierung**|
|----|----|----|
|im17|"Was man im Leben bekommt, hÃ¤ngt gar nicht so sehr von den eigenen Anstrengungen ab, sondern von der Wirtschaftslage, der Lage auf dem Arbeitsmarkt, den TarifabschlÃ¼ssen und den Sozialleistungen des Staates."| 1 = "Stimme voll zu" bis 4 = "Stimme Ã¼berhaupt nicht zu"|
|im18|"Das Einkommen sollte sich nicht allein nach der Leistung des Einzelnen richten. Vielmehr sollte jeder das haben, was er mit seiner Familie fÃ¼r ein anstÃ¤ndiges Leben braucht."| 1 = "Stimme voll zu" bis 4 = "Stimme Ã¼berhaupt nicht zu"|
|im19|"Nur wenn die Unterschiede im Einkommen und im sozialen Ansehen groÃŸ genug sind, gibt es auch einen Anreiz fÃ¼r persÃ¶nliche Leistungen."|1 = "Stimme voll zu" bis 4 = "Stimme Ã¼berhaupt nicht zu"|
|im20|"Die Rangunterschiede zwischen den Menschen sind akzeptabel, weil sie im Wesentlichen ausdrÃ¼cken, was man aus den Chancen, die man hatte, gemacht hat."| 1 = "Stimme voll zu" bis 4 = "Stimme Ã¼berhaupt nicht zu"|
|im21|"Ich finde die sozialen Unterschiede in unserem Land im GroÃŸen und Ganzen gerecht."|1 = "Stimme voll zu" bis 4 = "Stimme Ã¼berhaupt nicht zu"|

:::
:::

:::{.callout-tip collapse="true"}
# **LÃ¶sung**
```{r}
#| echo: true
#| warning: false

library(tidyverse)

df_rec<-df_csv|>
  select(gs01,educ,mstat,
         hs04,hs05,hs06,hs07,hs09,
         im17,im18,im19,im20,im21)

head(df_rec, 10)
```
:::

Nun schauen wir uns die Umkodierung von Faktoren am Beispiel des Wohnortes an:

:::{.panel-tabset}
# Alternative 1
```{r}

df_rec <- df_rec |>
  mutate(
    wohnort = recode(
      gs01,
      "1"= 1,
      "2" = 1,
      "3" = 1,
      "4" = 2,
      "5" = 2
    ))

df_rec |>
  select(gs01, wohnort) |>
  head(10)

```
:::{.callout-note collapse="true"}
# Beschreibung
Wir haben nun hier aus fÃ¼nf Gruppen zwei Gruppen gemacht. Wir unterscheiden nun zwischen **1 = Urban** und **2 = Rural**.
:::

# Alternative 2
```{r}

df_rec <- df_rec |>
  mutate(
    wohnort2 = factor(
      wohnort,
      levels = c(1, 2),
      labels = c("Urban", "Rural")
    )
  )

df_rec |>
  select(gs01, wohnort2) |>
  head(10)

```
::: {.callout-note collapse="true"}
# Beschreibung
Wir haben hier die zuvor erstellten Gruppen (`1` und `2`) nun als **Faktor mit Labeln** gemacht.
:::

# Alternative 3
```{r}

df_rec <- df_rec |>
  mutate(
    wohnort3 = recode(
      gs01,
      "1" = 1,
      "2" = 1,
      "3" = 1,
      "4" = 2,
      "5" = 2
    ) |> factor(labels = c("Urban", "Rural"))
  )

df_rec |>
  select(gs01, wohnort3) |>
  head(10)

```
:::{.callout-note collapse="true"}
# Beschreibung
Hier haben wir es in einem Schritt getan.
:::

# Alternative 4

```{r}

df_rec <- df_rec |>
  mutate(
    wohnort4 = recode(
      gs01,
      "1" = "Urban",
      "2" = "Urban",
      "3" = "Urban",
      "4" = "Rural",
      "5" = "Rural"
    ) |> factor(levels = c("Urban", "Rural"))
  )

df_rec |>
  select(gs01, wohnort4) |>
  head(10)

```
:::{.callout-note collapse="true"}
# Beschreibung
Hier haben wir erst "gelabled" und dann Levels hinzugefÃ¼gt.
:::
:::

## Ãœbung

Schaut euch die Variablen im neuen Datensatz `df_rec` an. Eure Aufgabe ist es nun neue Variablen zu erstellen, die sinnvoll sind und die wir dann weiter nutzen werden.

1. Erstellt eine neue Variable von `educ` (nennt diese `abschluss`) die vierstufig ist (niedriger, mittlere, hoher und anderer)
2. Erstellt einen Skalenwert fÃ¼r Distress und Soziale UnterstÃ¼tzung. Achtet darauf, dass der Summenwert sinnvol interpretierbar ist.

:::{.callout-tip collapse="true"}
# **Tipp**

- Schaut euch das Codebuch genauer an, um die Variablen besser zu verstehen.
- Guckt euch nochmal die Skalierung der Skalenitems an!
:::

::: {.callout-tip collapse="true"}
# **LÃ¶sung**

:::{.panel-tabset}

# Abschluss

```{r}

df_rec <- df_rec |>
  mutate(
    abschluss = recode(
      educ,
      "1" = 1,
      "2" = 1,
      "3" = 2,
      "4" = 3,
      "5" = 3,
      "6" = 4,
      "7" = 1
    ) |> factor(labels = c("Niedriger", "Mittlerer", "Hoher", "Anderer"))
  )

df_rec |>
  select(educ, abschluss) |>
  head(10)


```

# Distress

```{r}
#| echo: true
#| waring: false

library(sjmisc)

df_rec<-df_rec|>
  mutate(
    hs04_r = rec(hs04, rec = "rev"),
    hs05_r = rec(hs05, rec = "rev"),
    hs09_r = rec(hs09, rec = "rev"),
    disstress = hs04_r + hs05_r + hs06 + hs07+ hs09_r
  )

df_rec |>
  select(hs04, hs04_r, hs05, hs05_r, hs06, hs07, hs09, hs09_r, disstress)|>
  head(10)

```

# Soziale Ungleichheit

```{r}
#| echo: true
#| warning: false

df_rec<-df_rec|>
  mutate(
    im17_r = 5 - im17,
    im18_r = 5 - im18,
    im19_r = 5 - im19,
    im20_r = 5 - im20,
    im21_r = 5 - im21,
    soz.u = rowSums(cbind(im17_r,im18_r,im19_r,im20_r,im21_r))
  )

df_rec |>
  select(im17:im21, im17_r:im21_r, soz.u) |>
  head(10)

```

# Alles in einem Schritt

```{r}
#| echo: true
#| warning: false

df_rec<-df_rec|>
  mutate(
    abschluss = recode(
      educ,
      "1" = 1,
      "2" = 1,
      "3" = 2,
      "4" = 3,
      "5" = 3,
      "6" = 4,
      "7" = 1
    ) |> factor(labels = c("Niedriger", "Mittlerer", "Hoher", "Anderer")),
    hs04_r = rec(hs04, rec = "rev"),
    hs05_r = rec(hs05, rec = "rev"),
    hs09_r = rec(hs09, rec = "rev"),
    distress = hs04_r + hs05_r + hs06 + hs07+ hs09_r,
    im17_r = 5 - im17,
    im18_r = 5 - im18,
    im19_r = 5 - im19,
    im20_r = 5 - im20,
    im21_r = 5 - im21,
    soz.u = rowSums(cbind(im17_r,im18_r,im19_r,im20_r,im21_r))
  )

df_rec |>
  select(abschluss, disstress, soz.u) |>
  head(10)

```
:::
:::


# Joins â€“ DatensÃ¤tze zusammenfÃ¼hren
___
In der Praxis liegen Informationen hÃ¤ufig **nicht in einem einzigen Datensatz** vor. Stattdessen gibt es mehrere Tabellen, die sich auf **dieselben FÃ¤lle** beziehen, z. B.:

- ein Datensatz mit **Befragungsdaten**
- ein weiterer mit **Regionalinformationen**
- oder zusÃ¤tzliche **Kontextvariablen**

Um diese Informationen gemeinsam auswerten zu kÃ¶nnen, mÃ¼ssen wir die DatensÃ¤tze **zusammenfÃ¼hren**.  
Genau dafÃ¼r nutzen wir sogenannte **Joins**.

::: callout-tip
# Grundidee von Joins

Ein **Join** verbindet zwei DatensÃ¤tze Ã¼ber eine **gemeinsame Variable**  
(z. B. eine Personen-ID oder Fallnummer).

ğŸ‘‰ Voraussetzung:  
Beide DatensÃ¤tze enthalten **mindestens eine gemeinsame SchlÃ¼sselvariable**.
:::

In `dplyr` gibt es verschiedene Join-Typen, die sich darin unterscheiden, **welche FÃ¤lle im Ergebnis erhalten bleiben**.

:::{.callout-note collapse="true"}
# Die Join-Typen
- **left_join()** â†’ linker Datensatz bestimmt  
- **right_join()** â†’ rechter Datensatz bestimmt  
- **inner_join()** â†’ nur Ãœberschneidung  
- **full_join()** â†’ alles von beiden  
:::

Wir schauen uns nun den am **hÃ¤ufigsten** verwendeten Join-Befehl an: **`left_join()`**.
DafÃ¼r schauen wir uns nun den Befehl im Detail an.

:::{.callout-note}
# left_join()

left_join(x, y, by = "id")

- x â†’ **linker Datensatz**
- y â†’ **rechter Datensatz**
- by â†’ **SchlÃ¼sselvariable (hier "id")**
:::

Doch bevor wir nun DatensÃ¤tze zusammenfÃ¼hren, brauchen wir erstmal einen zweiten Datensatz.

â¡ï¸ **[Datensatz fÃ¼r Kinderarmut und Altersarmut nach BundeslÃ¤ndern](../ws1/assets/data/armutdaten.csv)**

Wir laden nun den neuen Datensatz in unser Environment.

```{r}
#| echo: true
#| warning: false

df_armut <- read_csv("assets/data/armutdaten.csv")

head(df_armut,10)

```

Nun schauen wir, ob wir eine SchlÃ¼sselvariable erkennen kÃ¶nnen. Die Variable `kzf` was die Kennziffer des **Bundeslandes** darstellt, kÃ¶nnte in Frage kommen. Welche Ã¤hnliche Variable haben wir in unserem **ersten Datensatz**?

:::{.callout-note collapse="true"}
# **Hinweis**
Die Variable `land` sieht doch vielversprechend aus, oder?

```{r}
#| echo: true
#| warning: false

df_spss |>
  select(land) |>
  head(10)

```
:::

Wir gucken uns die Kodierung im **Codebuch** an. Wir mÃ¼ssen vorher die Variable umstrukturieren. Erstelle in unserem **Datensatz** eine neue Variable mit dem namen `kzf`.

:::{.callout-tip collapse="true"}
# **LÃ¶sung**

```{r}
#| echo: true
#| warning: false

df_spss <- df_spss |>
  mutate(kzf = rec(land,rec="
                   10 = 1;
                   20 = 2;
                   30 = 3;
                   40 = 4;
                   50 = 5;
                   60 = 6;
                   70 = 7;
                   80 = 8;
                   90 = 9;
                   100 = 10;
                   111 = 11;
                   112 = 11;
                   120 = 12;
                   130 = 13;
                   140 = 14;
                   150 = 15;
                   160 = 16"))

df_spss |>
  select(kzf) |>
  head(10)

```
:::

Wir haben nun die **SchlÃ¼sselvariable** erstellt und finden sie in beiden **DatensÃ¤tzen**.

Bevor wir **die DatensÃ¤tze zusammenfÃ¼hre**, checken wir nochmal, ob in beiden DatensÃ¤tzen die **SchlÃ¼sselvariable** vorhanden ist.

```{r}
#| echo: true
#| warning: false

intersect(names(df_spss), names(df_armut))

```

Nun fÃ¼hren wir die **beiden DatensÃ¤tze** zusammen.

```{r}
#| echo: true
#| warning: false

df_spss_armut <- left_join(df_spss, df_armut, by = "kzf")

df_spss_armut |>
  select(66:70) |>
  head(10)

```

:::{.callout-note}
# **Hinweis**

Bei `select(66:70)` wÃ¤hle ich die Spalten **66 bis 70** in meinem neuen **Datensatz** aus.
:::

## Ãœbung

Erstellt erneut euren Datensatz aus der letzten Aufgabe und speichert ihn diesmal unter dem Namen **`df_rec2`**. Achtet darauf, dass ihr **eine eindeutige SchlÃ¼sselvariable** mitÃ¼bernehmt.

FÃ¼hrt anschlieÃŸend **dieselben Umkodierungen** durch wie zuvor bei `df_rec`.

Zum Schluss **fÃ¼gt ihr den Hauptdatensatz und `df_rec2` zu einem neuen Datensatz** mit dem Namen **`df_analyse`** zusammen.

:::{.callout-tip}
# **Tipp**
**Codebuch** ğŸ˜‰
:::

:::{.callout-tip collapse="true"}
# **LÃ¶sung**

```{r}
#| echo: true
#| warning: false

# Erstellen von df_rec2

df_rec2<-df_csv|>
  select(respid,gs01,educ,mstat,
         hs04,hs05,hs06,hs07,hs09,
         im17,im18,im19,im20,im21)

head(df_rec2, 10)

# Umkodieren und Summenwerte erstellen

df_rec2 <-df_rec2|>
  mutate(
    wohnort = recode(gs01,
      "1" = 1,
      "2" = 1,
      "3" = 1,
      "4" = 2,
      "5" = 2
    ) |> factor(labels = c("Urban", "Rural")),
    abschluss = recode(
      educ,
      "1" = 1,
      "2" = 1,
      "3" = 2,
      "4" = 3,
      "5" = 3,
      "6" = 4,
      "7" = 1
    ) |> factor(labels = c("Niedriger", "Mittlerer", "Hoher", "Anderer")),
    hs04_r = rec(hs04, rec = "rev"),
    hs05_r = rec(hs05, rec = "rev"),
    hs09_r = rec(hs09, rec = "rev"),
    distress = hs04_r + hs05_r + hs06 + hs07+ hs09_r,
    im17_r = 5 - im17,
    im18_r = 5 - im18,
    im19_r = 5 - im19,
    im20_r = 5 - im20,
    im21_r = 5 - im21,
    soz.u = rowSums(cbind(im17_r,im18_r,im19_r,im20_r,im21_r))
  )

df_rec2 |>
  select(15:26) |>
  head(10)

# Neuer Datensatz df_analyse

df_analyse <- left_join(df_csv, df_rec2, by="respid")

df_analyse |>
  select(81:91) |>
  head(10)
```
:::{.callout-tip collapse="true"}
# **Tip**
FÃ¼r eine saubere LÃ¶sung ohne Duplikate:
```{r}
#| echo: true
#| warning: false

dupl_vars <- setdiff(names(df_rec2), names(df_csv))

df_analyse <- df_csv |>
  left_join(df_rec2 |> select(respid, all_of(dupl_vars)), by = "respid")
```
:::

:::
:::{.callout-important collapse="true"}
# **Merkhilfe der Join-Befehle**

### `left_join()`

Der **linke Datensatz** ist die Basis.

- **alle FÃ¤lle** aus dem linken Datensatz bleiben erhalten  
- passende Informationen aus dem rechten Datensatz werden ergÃ¤nzt  
- nicht gefundene Werte werden als `NA` ergÃ¤nzt  

ğŸ‘‰ **Der am hÃ¤ufigsten verwendete Join** in der Praxis.

### `right_join()`

Der **rechte Datensatz** ist die Basis.

- **alle FÃ¤lle** aus dem rechten Datensatz bleiben erhalten  
- passende Informationen aus dem linken Datensatz werden ergÃ¤nzt  
- nicht gefundene Werte werden als `NA` ergÃ¤nzt  

ğŸ‘‰ Funktional das GegenstÃ¼ck zu `left_join()`.

### `inner_join()`

Es bleiben **nur die gemeinsamen FÃ¤lle** erhalten.

- FÃ¤lle kommen **nur dann** ins Ergebnis, wenn sie **in beiden DatensÃ¤tzen** vorkommen  
- alle anderen FÃ¤lle werden verworfen  

ğŸ‘‰ Strenger Join â€“ gut fÃ¼r saubere Ãœberschneidungen.

### `full_join()`

Alle FÃ¤lle aus **beiden DatensÃ¤tzen** bleiben erhalten.

- alle FÃ¤lle aus links **und** rechts  
- wo es keine Entsprechung gibt â†’ `NA`  

ğŸ‘‰ Maximale Information, aber oft viele fehlende Werte.

:::

# Fehlende Werte (NA)
___
Nachdem wir DatensÃ¤tze **umkodiert** und **zusammengefÃ¼hrt** haben, kommen wir zu einem Thema, das in der Praxis **immer** eine Rolle spielt:

ğŸ‘‰ **fehlende Werte (`NA`)**

Fehlende Werte entstehen zum Beispiel, wenn:

- Fragen *nicht* beantwortet wurden  
- bestimmte Fragen nur fÃ¼r *Teilgruppen* gestellt wurden  
- DatensÃ¤tze zusammengefÃ¼hrt werden  
- Filter oder Umkodierungen durchgefÃ¼hrt wurden

::: callout-tip
# **Wichtig**
Fehlende Werte sind **kein Fehler**, sondern ein **normaler Bestandteil realer Daten**.

Wichtig ist, zu verstehen,

- **wo** sie auftreten,
- **wie viele** es sind und
- **wie R mit ihnen umgeht**.
:::

## Was ist `NA`?

In R steht `NA` fÃ¼r **Not Available** â€“ also *fehlender Wert*.

- `NA` ist **keine** 0  
- `NA` ist **kein** leerer Text  
- `NA` bedeutet: ***Es liegt kein Wert vor***

R behandelt `NA` **sehr strikt** â€“ und das ist auch gut so.

## Fehlende Werte identifizieren

Bevor wir mit Berechnungen arbeiten, sollten wir immer prÃ¼fen, **ob und wo fehlende Werte** vorkommen. DafÃ¼r erstellen wir aus unserem **Analysedatensatz `df_analyse`** einen kleineren handlicheren Datensatz mit den Variablen die wir fÃ¼r unsere Analysen wirklich benÃ¶tigen und nennen es `df_analyse2`.

Nehme folgende Variablen mit rein:
- `respid`
- `wohnort`
- `isced97`
- `work`
- `dw15`
- `mstat`
- `hhincc`
- `disstress` und dazugehÃ¶rigen Variablen
- `soz.u` und dazugehÃ¶rigen Variablen

:::{.callout-tip collapse="true"}
# **LÃ¶sung**
```{r}
#| echo: true
#| warning: false

df_analyse2 <- df_analyse |>
  select(respid,age,sex,wohnort,isced97,work,dw15,
         mstat,hhincc,hs04_r,hs06,hs07,hs05_r,hs09_r,distress,
         im17_r,im18_r,im19_r,im20_r,im21_r,soz.u)

df_analyse2 |>
  head(10)
```
:::

:::{.callout-important collapse="true"}
# **Hinweis**
- Schaut im **Codebuch** nach, was die Variablen bedeuten.
- Achtet darauf, dass ihr die richtigen **Variablennamen** nehmt!
:::

Wir schauen nun unseren Datensatz an:

:::{.panel-tabset}

# Base R
```{r}
#| echo: true
#| warning: false
#| results: false

is.na(df_analyse2)

```
:::{.callout-caution}
# **Achtung**

Sehr **unÃ¼bersichtlich**, da R hier fÃ¼r **jede Zeile und jeder Variable** einen logischen Vektor mit `TRUE`oder `FALSE` erstellt.
:::

# **Package `naniar`**
```{r}
#| echo: true
#| warning: false

install.packages("naniar", repos="https://cloud.r-project.org")

library(naniar)

any_na(df_analyse2)
```
:::{.callout-caution}
# **Hinweis**
Mit diesem Befehl, bekommen wir eine schnelle RÃ¼ckmeldung, **ob** unser Datensatz fehlende Werte hat.
:::
:::


### Einzelne Variable prÃ¼fen
___
:::{.panel-tabset}

# Base R
```{r}
#| echo: true
#| warning: false
#| results: false

is.na(df_analyse2$age)

```
:::{.callout-caution}
# **Achtung**

Sehr **unÃ¼bersichtlich**, da R hier fÃ¼r **jede Zeile einen logischen Vektor mit `TRUE`oder `FALSE` erstellt.
:::

# **Package `naniar`**
```{r}
#| echo: true
#| warning: false


any_na(df_analyse2$age)

```
:::{.callout-caution}
# **Hinweis**
Mit diesem Befehl, bekommen wir eine schnelle RÃ¼ckmeldung, **ob** in er spezifischen Variable fehlende Werte hat.
:::
:::

Um herauszufinden, **wie viele fehlende Werte** die Variable hat, gehen wir wie folgt vor:

:::{.panel-tabset}

# Base R
```{r}
#| echo: true
#| warning: false

sum(is.na(df_analyse2$age))

```

# **Package `naniar`**
```{r}
#| echo: true
#| warning: false


n_miss(df_analyse2$age)

```

:::


### Ãœberblick mehrer Variablen
___
Wir kÃ¶nnen auch **mehrere spezifische** Variablen anschauen:

:::{.panel-tabset}
# Base R

```{r}
df_analyse2 |>
  summarise(
    across(
      c(distress, soz.u),
      ~ sum(is.na(.))
    )
  )
```
::: callout-note
Hier sehen wir auf einen Blick, wie viele fehlende Werte pro Variable existieren.
:::

# Ãœberblick (grafisch)

```{r}
#| echo: true
#| warning: false

gg_miss_var(df_analyse2)


```
::: callout-note
Hier sehen wir auf einen Blick, wie viele fehlende Werte pro Variable im **gesamten Datensatz** sind.
:::
:::

## Was passiert bei Berechnungen mit `NA`?

Ein hÃ¤ufiger Stolperstein ist der Umgang mit fehlenden Werten (`NA`) bei Berechnungen.

ğŸ‘‰ Schon ein einziges `NA` reicht aus, damit das gesamte Ergebnis `NA` wird.

```{r}
#| echo: true
#| warning: false


rowSums(cbind(1,5,8,10,NA,20,12,13,NA))


1+5+8+10+NA+20+12+13+NA

```

### Warum ist das so?
___
R kann nicht erraten, wie mit fehlenden Werten umgegangen werden soll:

- Soll das NA ignoriert werden?
- Soll das Ergebnis trotzdem berechnet werden?
- Oder ist das Ergebnis nicht sinnvoll interpretierbar?

Ohne eine klare Anweisung entscheidet sich R fÃ¼r die **sichere Variante** und gibt `NA` zurÃ¼ck.

Das gilt fÃ¼r viele Funktionen, zum Beispiel:

- `sum()`
- `mean()`
- `rowSums()`
- `rowMeans()`

ğŸ‘‰ Wichtig: Wenn fehlende Werte ignoriert werden sollen, muss man das **explizit angeben**.

```{r}
#| echo: true
#| warning: false

rowSums(cbind(1,5,8,10,NA,20,12,13,NA), na.rm = TRUE)

```
:::{.callout-important}
# **Was bedeutet `na.rm`?**

`na.rm = TRUE` bedeutet:  
**fehlende Werte (`NA`) werden bei der Berechnung ignoriert**.
:::

`na.rm = TRUE` entfernt `NA`-Werte aus der Berechnung, damit Summen trotzdem berechnet werden kÃ¶nnen.

```{r}
#| echo: true
#| warning: false

rowSums(cbind(1,5,8,10,NA,20,12,13,NA), na.rm = TRUE)

1+5+8+10+20+12+13

```

Wir wissen, dass wir auf einigen Skalen, die wir gebildet haben, viele Missings haben.

```{r}
#| echo: true
#| warning: false

gg_miss_var(df_analyse2)

```

Wir schauen uns das mal genauer an:
```{r}
#| echo: true
#| warning: false

df_analyse2 <- df_analyse2 |>
  mutate(
    soz.u_na = rowSums(cbind(im17_r, im18_r, im19_r, im20_r, im21_r)),
    soz.u_rmt = rowSums(
      cbind(im17_r, im18_r, im19_r, im20_r, im21_r),
      na.rm = TRUE)
  )

df_analyse2 |>
  select(soz.u_na, soz.u_rmt) |>
  gg_miss_var()

```

:::{.callout-warning}
# **Achtung**

`na.rm = TRUE` bedeutet nicht automatisch â€besserâ€œ.

Wenn sehr viele Items fehlen, kann ein **Skalenwert inhaltlich verzerrt** sein!
:::

### Wann ist na.rm = TRUE sinnvoll?
___
- wenn nur wenige Werte fehlen
- wenn ein Skalenwert auch mit 1â€“2 fehlenden Items sinnvoll ist
- fÃ¼r erste Auswertungen und Exploration

## Wie mit fehlenden Werten umgehen?

In der empirischen Forschung gibt es **verschiedene Strategien**, um mit fehlenden Werten umzugehen.  
Welche sinnvoll ist, hÃ¤ngt unter anderem ab von:

- der Anzahl der fehlenden Werte  
- dem Datentyp (Item, Skala, Hintergrundvariable)  
- der Fragestellung der Analyse  

MÃ¶gliche AnsÃ¤tze sind zum Beispiel:

- fehlende Werte **ignorieren** (`na.rm = TRUE`)
- FÃ¤lle mit fehlenden Werten **ausschlieÃŸen**
- fehlende Werte **schÃ¤tzen oder imputieren** (z. B. Mittelwert, multiple Imputation)

ğŸ‘‰ **Wichtig:**  
Diese Entscheidungen haben **inhaltliche Konsequenzen** und sollten immer bewusst getroffen werden.


### Fokus im Workshop
___
In diesem Workshop konzentrieren wir uns bewusst auf einen **einfachen und transparenten Umgang** mit fehlenden Werten:

ğŸ‘‰ **FÃ¤lle mit fehlenden Werten werden ausgeschlossen**, wenn sie fÃ¼r die jeweilige Analyse nicht sinnvoll nutzbar sind.

Warum?

- leicht nachvollziehbar  
- gut geeignet fÃ¼r den Einstieg 
- hÃ¤ufig ausreichend bei groÃŸen Stichproben  

Komplexere Verfahren wie **Imputation** sind wichtig, gehen aber Ã¼ber den Rahmen dieses Workshops hinaus. Es lohnt sich aber hier **methodisch** einzuarbeiten!

ğŸ‘‰ **Multiple Imputation**

## Fehlende Werte entfernen

Wie oben angesprochen, entfernen wir bei **ausreichender Datengrundlage** die Personen, die einen fehlende Werte auf die uns wichtigen Variablen haben.


### Ganze Zeilen entfernen (`drop_na()`)
___
Mit `drop_na()` entfernen wir **alle Zeilen**, in denen mindestens ein `NA` in den angegebenen Variablen vorkommt.

```{r}
#| echo: true
#| warning: false

df_clean <- df_analyse2 |>
  drop_na(soz.u_na)

df_clean |>
  dim()

  df_analyse2 |>
  dim()
```

ğŸ‘‰ Ergebnis: Nur FÃ¤lle ohne fehlende Werte in `soz.u` bleiben erhalten.

:::{.callout-tip collapse="true"}
# Tipp
Um zu schauen, wie viele Personen damit ausgeschlossen wurden, kann man folgendes machen:

```{r}
#| echo: true
#| warning: false

nrow(df_analyse2) - nrow(df_clean)

```
:::

## Ãœbung

Erstelle einen Datensatz `df_clean`, welcher frei von `NA's` ist fÃ¼r die Variablen `distress` und `soz.u`. 

- Wie viele Beobachtungen werden final ausgeschlossen?

:::{.callout-tip collapse="true"}
# **LÃ¶sung**

```{r}
#| echo: true
#| warning: false

df_analyse2 <- df_analyse2 |>
  mutate(
    distress_na = rowSums(cbind(hs04_r,hs06,hs07,hs05_r,hs09_r)),
    distress_rmt = rowSums(
      cbind(hs04_r,hs06,hs07,hs05_r,hs09_r),
      na.rm = TRUE)
  )

df_clean <- df_analyse2 |>
  drop_na(distress_na, soz.u_na)

nrow(df_analyse2) - nrow(df_clean) 

```
:::

:::{.callout-tip}
# **Tipps**

:::{.panel-tabset}
# **Gute Praxis**

Nach dem Entfernen von `NA` immer prÃ¼fen:

- Wie viele FÃ¤lle sind Ã¼brig?
- Ist die Fallzahl noch ausreichend?
- Betrifft das Entfernen bestimmte Gruppen stÃ¤rker als andere?

# **Merksatz**

Es gibt keinen automatisch richtigen Umgang mit fehlenden Werten. Wichtig ist,

- bewusst zu entscheiden,
- transparent zu dokumentieren,
- und die Konsequenzen zu kennen.
:::
:::


# Recap Workshop
___
Bevor wir ins **Mini-Projekt** starten, fassen wir kurz zusammen, was wir an **Tag 1** und **Tag 2** gemacht haben.

:::{.callout-note}
# **RECAPS**

:::{.panel-tabset}
# **Recap â€“ Tag 1 (Grundlagen)**

- **R vs. RStudio**: Was ist die Sprache, was ist die OberflÃ¤che?
- **RStudio verstehen**: Script, Console, Environment, Files/Plots/Help
- **Projekte anlegen**: Warum Projekte wichtig sind (Pfade, Ordnung, Reproduzierbarkeit)
- **R-Skripte nutzen**: Code speichern, wiederverwenden, nachvollziehbar arbeiten
- **Objekte & Zuweisung**: `<-`, Vektoren mit `c()`, Werte speichern
- **Funktionen**: `mean()`, `sd()`, `sum()`, `length()` usw. + Kombinationen
- **Datentypen & Strukturen**: numeric/character/factor und Vektor vs. Data Frame
- **Datensatzstruktur**: Zugriff auf Variablen mit `$` und erste Kennwerte
- **Pakete**: `install.packages()` vs. `library()` (tidyverse, haven, psych)
- **Datenimport**: CSV und SPSS (`read_csv()`, `read_sav()`)
- **PlausibilitÃ¤tscheck**: `summary()` und `glimpse()`
- **Datenaufbereitung mit dplyr**: `select()`, `filter()`, `mutate()` + neue Objekte
- **Pipes** (`|>`): Schritte lesbar als Workflow â€hintereinanderâ€œ schreiben

# **Recap â€“ Tag 2 (Datenaufbereitung)**

- **Umkodieren**:
  - invertierte Items (Skalen in gleiche Richtung bringen)
  - Gruppen/Faktoren zusammenfassen (z. B. Wohnort â†’ Urban/Rural)
  - Labels / Faktoren erstellen (Interpretation verbessern)
- **Skalenbildung**:
  - Summenwerte erstellen (z. B. `rowSums()` fÃ¼r mehrere Items)
  - Hinweis: Richtung & Interpretation vorher festlegen
- **Joins (DatensÃ¤tze zusammenfÃ¼hren)**:
  - SchlÃ¼sselvariablen identifizieren/erstellen
  - `left_join()` als Standardfall, plus Ãœberblick: right/inner/full
- **Fehlende Werte (NA)**:
  - NA erkennen und zÃ¤hlen (`any_na()`, `n_miss()`, `gg_miss_var()`)
  - verstehen, was NA bei Berechnungen macht
  - `na.rm = TRUE` als explizite Anweisung zum Ignorieren
  - `drop_na()` als einfacher Workflow-Schritt fÃ¼r â€saubereâ€œ Analysedaten
:::
:::

## Workflow

Ziel ist ein **gut dokumentierter und jederzeit nachvollziehbarer Analyse-Workflow**:  
Jemand anderes (oder du in 3 Monaten) soll verstehen, **was** du gemacht hast, **warum** du es gemacht hast und **wie** du zu deinen Ergebnissen kommst.

:::{.panel-tabset}

# Projektstruktur

Eine mÃ¶gliche Struktur:

- `projektname/`  
  - `data/`  *(Rohdaten & ggf. bereinigte Daten)*  
  - `scripts/`  *(alle R-Skripte)*  
  - `output/`  *(Tabellen, Abbildungen, Reports)*  
  - `projektname.Rproj`

# Skripte sinnvoll gliedern

Statt ein langes Skript:

- `01_import.R`  *(Daten laden, erste Checks)*  
- `02_cleaning.R`  *(Umkodieren, Skalen, Missing neue Variablen)*  
- `03_analysis.R`  *(Modelle, Tests, Kennwerte)*  
- `04_plots.R`  *(Visualisierungen, Export)*  

So kannst du jeden Schritt einzeln ausfÃ¼hren, testen und bei Fehlern gezielt zurÃ¼ckspringen.

# Kommentare nutzen

Kommentiere kurz, **was** passiert und **warum**:

- Was wird hier gemacht?  
- Welche Annahme steckt dahinter?  
- Welche Variablen/Skalen sind wichtig?

Beispiel:

```{r}
# Invertiere hs04/hs05/hs09, damit hohe Werte = mehr Distress bedeuten
# Skala: 1â€“5, deshalb: 6 - Wert
```
:::

## **Exkurs: Datensatz Speichern**

Nach der Datenaufbereitung oder Analyse ist es sinnvoll, **Zwischenergebnisse zu speichern** (z. B. bereinigte DatensÃ¤tze oder finale Analyse-Daten).

So kannst du spÃ¤ter direkt weiterarbeiten, ohne alle Schritte erneut ausfÃ¼hren zu mÃ¼ssen.

:::{.panel-tabset}
# CSV

- plattformunabhÃ¤ngig
- gut fÃ¼r Weitergabe und Dokumentation

```{r}
#| echo: true
#| warning: false

write_csv(df_clean, file = "assets/data/output/Datensatz_clean.csv")

```

# SPSS

- praktisch fÃ¼r Zusammenarbeit mit SPSS-Nutzer:innen
- Variablenlabels bleiben erhalten

```{r}
#| echo: true
#| warning: false

write_sav(df_clean, path = "assets/data/output/spss_Datensatz_clean.sav")

```
:::


# Mini-Projekt
___
Zum Abschluss setzen wir alles, was wir in Tag 1 und Tag 2 gelernt haben, in einem kleinen, realistischen Workflow zusammen.

**Zielsetzung**

Am Ende dieses Mini-Projekts kÃ¶nnt ihr:

- einen Datensatz **importieren** und schnell **prÃ¼fen** (`glimpse()`, `summary()`)
- Variablen **auswÃ¤hlen, umkodieren und neue Variablen erstellen** (`select()`, `mutate()`, `recode()`)
- eine Skala eurer Wahl bilden **(inkl. ggf. Invertierung)**
- **fehlende Werte erkennen** und sinnvoll damit umgehen (`any_na()`, `n_miss()`, `na.rm`, `drop_na()`)
- einen **zweiten Datensatz** joinen (`left_join()`)
- einen **â€Analyse-Datensatzâ€œ erstellen** und **speichern** (`write_csv()`, optional `write_sav()`)
- eine **kleine** deskriptive Auswertung machen (z. B. **Mittelwerte / SD / HÃ¤ufigkeiten**)

## Aufgabenstellung

Ihr erstellt euch einen eigenen Datensatz df_mini, der am Ende â€analysefertigâ€œ ist.

**Schritt 1: Datensatz laden und Ãœberblick verschaffen**

- lade **df_csv** (und ggf. df_spss)
- verschaffe dir einen **Ãœberblick**:
  - `glimpse()`
  - `summary()`
___
**Schritt 2: Skala auswÃ¤hlen (Codebuch!)**

WÃ¤hlt eine Skala eurer Wahl aus dem Codebuch:

- Skala aus **mindestens 3 Items**
- achtet auf die **Skalierung** (z. B. 1â€“4 oder 1â€“5)
- prÃ¼ft, ob es **invertierte** Items gibt (**und entscheidet, was â€hochâ€œ bedeuten soll**)

**ğŸ‘‰ Speichert eure Entscheidung kurz als Kommentar im Skript**

```{r}
#| echo: true
#| warning: false

# Bildung der Skala _______ (Items: __, __, __)
## Hohe Werte sollen _______ bedeuten.

```
___
**Schritt 3: Datensatz erstellen (Auswahl der Variablen)**

Erstellt `df_mini` mit:

- `respid` (**SchlÃ¼sselvariable!**)
- mindestens 2 **demographische Variablen** (z. B. `age`, `sex`, `work`, `mstat`, `wohnort`)
- alle Items eurer ausgewÃ¤hlten Skala
___
**Schritt 4: Umkodieren & Skalenwert bilden**

- falls nÃ¶tig: *invertiert Items*
- bildet einen **Summen- oder Mittelwert**:
  - `rowSums(..., na.rm = TRUE)` *oder*
  - `rowMeans(..., na.rm = TRUE)`

ğŸ‘‰ Gebt der Skala einen sinnvollen Namen (z. B. `skala_x_sum` oder `skala_x_mean`)
___
**Schritt 5: Missing Values kurz checken**

- **prÃ¼ft:** Gibt es `NA` im Datensatz?
- **prÃ¼ft:** Wie viele `NA` in eurer Skala?
- entscheidet euch fÃ¼r einen e**infachen Schritt**:
  - entweder `na.rm = TRUE` in der Skala reicht **oder**
  - `drop_na()` fÃ¼r die Skala (**fÃ¼r â€saubereâ€œ Analyse**)
___
**Schritt 6: Join anwenden**

FÃ¼hrt einen `left_join()` mit dem Armutsdatensatz durch (wie zuvor):

- **SchlÃ¼sselvariable** anpassen und erstellen (falls nÃ¶tig)
- danach **prÃ¼fen**, ob neue Variablen angekommen sind

:::{.callout-tip collapse="true"}
# **Optional andere Kontextvariablen**

Auf der Seite von [INKAR](https://www.inkar.de/) kÃ¶nnt ihr andere Kontextvariablen euch aussuchen und auf Bundeslandebene runterladen.

:::
___
**Schritt 7: Datensatz speichern**

**Speichert** euren finalen Datensatz:

- als CSV (`write_csv()`)
- **optional** als SPSS (`write_sav()`)

## Mini-Analyse: Deskriptive Statistik

Erstellt zum Schluss eine kurze deskriptive Auswertung:

**Hintergrundvariablen (Beispiele)**

- `age`: Mittelwert + SD
- `sex` oder `work` oder `abschluss`: **HÃ¤ufigkeiten**

:::{.callout-note collapse="true"}
# **Tipp: Deskriptive Statistik**

Schaut euch die Funktionen aus dem `psych` und dem `janitor`Packages an!

- `install.packages("janitor", repos = "https://cloud.r-project.org")`
- `install.packages("psych", repos = "https://cloud.r-project.org")`

**Kategorische Variablen**

- `janitor::tabyl()`

**Numerische Variablen**

- `psych::describe()`

:::

**Eure Skala**

- Mittelwert + SD
- Min/Max (**optional**)
- Missing-Anteil (**optional**)

## Checkliste

Am Ende sollten bei euch vorhanden sein:

- ein Datensatz `df_mini` (oder `df_mini_clean`)
- eine selbst gebildete Skala (`*_sum` oder `*_mean`)
- ein kurzer Deskriptiv-Output (**Mittelwert/SD + HÃ¤ufigkeiten**)
- ein gespeicherter Output (`.csv`, optional `.sav`)

# Abschluss Tag 2
___
Danke, dass ihr heute wieder dabei wart ğŸ‘‹  
Ihr habt jetzt einen vollstÃ¤ndigen **Basis-Workflow** kennengelernt â€“ von Daten einlesen bis zu ersten "Auswertungen".

Bevor wir abschlieÃŸen:  
ğŸ‘‰ Gibt es noch offene **Fragen** oder Stellen, die wir **gemeinsam nochmal anschauen sollen**?

:::{.callout-note}
# **Merksatz zum Abschluss**
Guter Code ist nicht der â€kÃ¼rzesteâ€œ â€“ sondern der, den du (und andere) spÃ¤ter noch verstehst.
:::

## **Always Remember**

![](assets/images/rmeme.jpg){width=100% style="border: 2px solid darkblue;"}

___
[â¬…ï¸ ZurÃ¼ck zu Tag 1](day1.qmd){.btn .btn-primary}